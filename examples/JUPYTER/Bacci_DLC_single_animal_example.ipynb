{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4552f2ad",
   "metadata": {},
   "source": [
    "# DeepLabCut example for single animal project - BacciLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2884282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc13...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import deeplabcut\n",
    "\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "%gui qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c848db",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r\"\" # for creating a new project\n",
    "project_location = r\"\" # for creating a new project\n",
    "\n",
    "config_path = r\"\" # to use an existing project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1932fb",
   "metadata": {},
   "source": [
    "### Create a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c723cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = deeplabcut.create_new_project(\n",
    "    project='myproject',\n",
    "    experimenter='experiementer',\n",
    "    videos=list(Path(video_path).glob('*.mp4')),\n",
    "    working_directory=Path(project_location),\n",
    "    multianimal=False,\n",
    "    copy_videos=False,\n",
    ")\n",
    "\n",
    "config_path = Path(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ba205",
   "metadata": {},
   "source": [
    "### Extracting Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae955eaa",
   "metadata": {},
   "source": [
    "For automatic frame extraction, use cell first below. Thre is two algorithm available uniform (faster) and kmeans (clustering) which would be recommended only if videos are short. It is recommended to extract frames automatically the first time for convenience. It will be always possible to complete training set by extracting frames manually (second cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cd2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting frames automatically\n",
    "deeplabcut.extract_frames(\n",
    "    config_path,\n",
    "    mode='automatic',\n",
    "    algo='uniform',\n",
    "    userfeedback=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1287fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting frames manually\n",
    "video_path = r\"video.mp4\"\n",
    "deeplabcut.extract_frames(\n",
    "    config_path, \n",
    "    \"manual\", \n",
    "    videos_list=[video_path]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8c5d7",
   "metadata": {},
   "source": [
    "### Labelling Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78677b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling frames of a specific folder\n",
    "image_folder = r\"myproject\\labeled-data\\video\"\n",
    "_ = deeplabcut.label_frames(config_path, image_folder=image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6250ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling frames of all folders in a loop\n",
    "labeled_data_dir = os.path.join(os.path.dirname(config_path), 'labeled-data')\n",
    "labeled_folders = [foldername for foldername in os.listdir(labeled_data_dir) if '_labeled' not in foldername]\n",
    "\n",
    "deeplabcut.label_frames_in_loop(config_path, labeled_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1eb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.SkeletonBuilder(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebe620",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.check_labels(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c5826",
   "metadata": {},
   "source": [
    "### Create the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9613951",
   "metadata": {},
   "source": [
    "Choose a proper backbone for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b712ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_training_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnum_shuffles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mShuffles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mwindows2linux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muserfeedback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainIndices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtestIndices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnet_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdetector_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0maugmenter_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mposecfg_template\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msuperanimal_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweight_init\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'WeightInitialization | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Engine | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mctd_conditions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | str | Path | tuple[int, str] | tuple[int, int] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Creates a training dataset.\n",
      "\n",
      "Labels from all the extracted frames are merged into a single .h5 file.\n",
      "Only the videos included in the config file are used to create this dataset.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "config : string\n",
      "    Full path of the ``config.yaml`` file as a string.\n",
      "\n",
      "num_shuffles : int, optional, default=1\n",
      "    Number of shuffles of training dataset to create, i.e. ``[1,2,3]`` for\n",
      "    ``num_shuffles=3``.\n",
      "\n",
      "Shuffles: list[int], optional\n",
      "    Alternatively the user can also give a list of shuffles.\n",
      "\n",
      "userfeedback: bool, optional, default=True\n",
      "    If ``False``, all requested train/test splits are created (no matter if they\n",
      "    already exist). If you want to assure that previous splits etc. are not\n",
      "    overwritten, set this to ``True`` and you will be asked for each split.\n",
      "\n",
      "trainIndices: list of lists, optional, default=None\n",
      "    List of one or multiple lists containing train indexes.\n",
      "    A list containing two lists of training indexes will produce two splits.\n",
      "\n",
      "testIndices: list of lists, optional, default=None\n",
      "    List of one or multiple lists containing test indexes.\n",
      "\n",
      "net_type: list, optional, default=None\n",
      "    Type of networks. The options available depend on which engine is used.\n",
      "    Currently supported options are:\n",
      "        TensorFlow\n",
      "            * ``resnet_50``\n",
      "            * ``resnet_101``\n",
      "            * ``resnet_152``\n",
      "            * ``mobilenet_v2_1.0``\n",
      "            * ``mobilenet_v2_0.75``\n",
      "            * ``mobilenet_v2_0.5``\n",
      "            * ``mobilenet_v2_0.35``\n",
      "            * ``efficientnet-b0``\n",
      "            * ``efficientnet-b1``\n",
      "            * ``efficientnet-b2``\n",
      "            * ``efficientnet-b3``\n",
      "            * ``efficientnet-b4``\n",
      "            * ``efficientnet-b5``\n",
      "            * ``efficientnet-b6``\n",
      "        PyTorch (call ``deeplabcut.pose_estimation_pytorch.available_models()`` for\n",
      "        a complete list)\n",
      "            * ``animaltokenpose_base``\n",
      "            * ``cspnext_m``\n",
      "            * ``cspnext_s``\n",
      "            * ``cspnext_x``\n",
      "            * ``ctd_coam_w32``\n",
      "            * ``ctd_coam_w48``\n",
      "            * ``ctd_prenet_cspnext_m``\n",
      "            * ``ctd_prenet_cspnext_x``\n",
      "            * ``ctd_prenet_rtmpose_x_human``\n",
      "            * ``ctd_prenet_hrnet_w32``\n",
      "            * ``ctd_prenet_hrnet_w48``\n",
      "            * ``ctd_prenet_rtmpose_m``\n",
      "            * ``ctd_prenet_rtmpose_x``\n",
      "            * ``ctd_prenet_rtmpose_x_human``\n",
      "            * ``dekr_w18``\n",
      "            * ``dekr_w32``\n",
      "            * ``dekr_w48``\n",
      "            * ``dlcrnet_stride16_ms5``\n",
      "            * ``dlcrnet_stride32_ms5``\n",
      "            * ``hrnet_w18``\n",
      "            * ``hrnet_w32``\n",
      "            * ``hrnet_w48``\n",
      "            * ``resnet_101``\n",
      "            * ``resnet_50``\n",
      "            * ``rtmpose_m``\n",
      "            * ``rtmpose_s``\n",
      "            * ``rtmpose_x``\n",
      "            * ``top_down_cspnext_m``\n",
      "            * ``top_down_cspnext_s``\n",
      "            * ``top_down_cspnext_x``\n",
      "            * ``top_down_hrnet_w18``\n",
      "            * ``top_down_hrnet_w32``\n",
      "            * ``top_down_hrnet_w48``\n",
      "            * ``top_down_resnet_101``\n",
      "            * ``top_down_resnet_50``\n",
      "\n",
      "detector_type: string, optional, default=None\n",
      "    Only for the PyTorch engine.\n",
      "    When passing creating shuffles for top-down models, you can specify which\n",
      "    detector you want. If the detector_type is None, the ```ssdlite``` will be used.\n",
      "    The list of all available detectors can be obtained by calling\n",
      "    ``deeplabcut.pose_estimation_pytorch.available_detectors()``. Supported options:\n",
      "        * ``ssdlite``\n",
      "        * ``fasterrcnn_mobilenet_v3_large_fpn``\n",
      "        * ``fasterrcnn_resnet50_fpn_v2``\n",
      "\n",
      "augmenter_type: string, optional, default=None\n",
      "    Type of augmenter. The options available depend on which engine is used.\n",
      "    Currently supported options are:\n",
      "        TensorFlow\n",
      "            * ``default``\n",
      "            * ``scalecrop``\n",
      "            * ``imgaug``\n",
      "            * ``tensorpack``\n",
      "            * ``deterministic``\n",
      "        PyTorch\n",
      "            * ``albumentations``\n",
      "\n",
      "posecfg_template: string, optional, default=None\n",
      "    Only for the TensorFlow engine.\n",
      "    Path to a ``pose_cfg.yaml`` file to use as a template for generating the new\n",
      "    one for the current iteration. Useful if you would like to start with the same\n",
      "    parameters a previous training iteration. None uses the default\n",
      "    ``pose_cfg.yaml``.\n",
      "\n",
      "superanimal_name: string, optional, default=\"\"\n",
      "    Only for the TensorFlow engine. For the PyTorch engine, use the ``weight_init``\n",
      "    parameter.\n",
      "    Specify the superanimal name is transfer learning with superanimal is desired.\n",
      "    This makes sure the pose config template uses superanimal configs as template.\n",
      "\n",
      "weight_init: WeightInitialisation, optional, default=None\n",
      "    PyTorch engine only. Specify how model weights should be initialized. The\n",
      "    default mode uses transfer learning from ImageNet weights.\n",
      "\n",
      "engine: Engine, optional\n",
      "    Whether to create a pose config for a Tensorflow or PyTorch model. Defaults to\n",
      "    the value specified in the project configuration file. If no engine is specified\n",
      "    for the project, defaults to ``deeplabcut.compat.DEFAULT_ENGINE``.\n",
      "\n",
      "ctd_conditions: int | str | Path | tuple[int, str] | tuple[int, int] | None, default = None,\n",
      "    If using a conditional-top-down (CTD) net_type, this argument should be\n",
      "    specified. It defines the conditions that will be used with the CTD model.\n",
      "    It can be either:\n",
      "        * A shuffle number (ctd_conditions: int), which must correspond to a\n",
      "            bottom-up (BU) network type.\n",
      "        * A predictions file path (ctd_conditions: string | Path), which must\n",
      "            correspond to a .json or .h5 predictions file.\n",
      "        * A shuffle number and a particular snapshot\n",
      "            (ctd_conditions: tuple[int, str] | tuple[int, int]), which respectively\n",
      "            correspond to a bottom-up (BU) network type and a particular snapshot\n",
      "            name or index.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "list(tuple) or None\n",
      "    If training dataset was successfully created, a list of tuples is returned.\n",
      "    The first two elements in each tuple represent the training fraction and the\n",
      "    shuffle value. The last two elements in each tuple are arrays of integers\n",
      "    representing the training and test indices.\n",
      "\n",
      "    Returns None if training dataset could not be created.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Use the function ``add_new_videos`` at any stage of the project to add more videos\n",
      "to the project.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Linux/MacOS:\n",
      ">>> deeplabcut.create_training_dataset(\n",
      "        '/analysis/project/reaching-task/config.yaml', num_shuffles=1,\n",
      "    )\n",
      "\n",
      ">>> deeplabcut.create_training_dataset(\n",
      "        '/analysis/project/reaching-task/config.yaml', Shuffles=[2], engine=deeplabcut.Engine.TF,\n",
      "    )\n",
      "\n",
      "Windows:\n",
      ">>> deeplabcut.create_training_dataset(\n",
      "        'C:\\Users\\Ulf\\looming-task\\config.yaml', Shuffles=[3,17,5],\n",
      "    )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\mai-an.nguyen\\work\\deeplabcut\\deeplabcut\\generate_training_dataset\\trainingsetmanipulation.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(\n",
    "    config_path,\n",
    "    net_type='top_down_hrnet_w32'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b9e1a",
   "metadata": {},
   "source": [
    "Go to the pytorch_config.yaml to tune training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d67c4",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e98098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | Path'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdisplayiters\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msaveiters\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxiters\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msave_epochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mallow_growth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgputouse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mautotune\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mkeepdeconvweights\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msuperanimal_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msuperanimal_transfer_learning\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Engine | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msnapshot_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | Path | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdetector_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | Path | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdetector_batch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdetector_epochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdetector_save_epochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpose_threshold\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'float | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpytorch_cfg_updates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'dict | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Trains the network with the labels in the training dataset.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "config : string\n",
      "    Full path of the config.yaml file as a string.\n",
      "\n",
      "shuffle: int, optional, default=1\n",
      "    Integer value specifying the shuffle index to select for training.\n",
      "\n",
      "trainingsetindex: int, optional, default=0\n",
      "    Integer specifying which TrainingsetFraction to use.\n",
      "    Note that TrainingFraction is a list in config.yaml.\n",
      "\n",
      "max_snapshots_to_keep: int or None\n",
      "    Sets how many snapshots are kept, i.e. states of the trained network. Every\n",
      "    saving iteration many times a snapshot is stored, however only the last\n",
      "    ``max_snapshots_to_keep`` many are kept! If you change this to None, then all\n",
      "    are kept.\n",
      "    See: https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835\n",
      "\n",
      "displayiters: optional, default=None\n",
      "    This variable is actually set in ``pose_config.yaml``. However, you can\n",
      "    overwrite it with this hack. Don't use this regularly, just if you are too lazy\n",
      "    to dig out the ``pose_config.yaml`` file for the corresponding project. If\n",
      "    ``None``, the value from there is used, otherwise it is overwritten!\n",
      "\n",
      "saveiters: optional, default=None\n",
      "    Only for the TensorFlow engine (for the PyTorch engine see the ``torch_kwargs``:\n",
      "    you can use ``save_epochs``).\n",
      "    This variable is actually set in ``pose_config.yaml``. However, you can\n",
      "    overwrite it with this hack. Don't use this regularly, just if you are too lazy\n",
      "    to dig out the ``pose_config.yaml`` file for the corresponding project.\n",
      "    If ``None``, the value from there is used, otherwise it is overwritten!\n",
      "\n",
      "maxiters: optional, default=None\n",
      "    Only for the TensorFlow engine (for the PyTorch engine see the ``torch_kwargs``:\n",
      "    you can use ``epochs``).\n",
      "    This variable is actually set in ``pose_config.yaml``. However, you can\n",
      "    overwrite it with this hack. Don't use this regularly, just if you are too lazy\n",
      "    to dig out the ``pose_config.yaml`` file for the corresponding project.\n",
      "    If ``None``, the value from there is used, otherwise it is overwritten!\n",
      "\n",
      "epochs: optional, default=None\n",
      "    Only for the PyTorch engine (equivalent to the `maxiters` parameter for the\n",
      "    TensorFlow engine). The maximum number of epochs to train the model for. If\n",
      "    None, the value will be read from the `pytorch_config.yaml` file. An epoch is a\n",
      "    single pass through the training dataset, which means your model has seen each\n",
      "    training image exactly once. So if you have 64 training images for your network,\n",
      "    an epoch is 64 iterations with batch size 1 (or 32 iterations with batch size 2,\n",
      "    16 with batch size 4, etc.).\n",
      "\n",
      "save_epochs: optional, default=None\n",
      "    Only for the PyTorch engine (equivalent to the `saveiters` parameter for the\n",
      "    TensorFlow engine). The number of epochs between each snapshot save. If\n",
      "    None, the value will be read from the `pytorch_config.yaml` file.\n",
      "\n",
      "allow_growth: bool, optional, default=True.\n",
      "    Only for the TensorFlow engine.\n",
      "    For some smaller GPUs the memory issues happen. If ``True``, the memory\n",
      "    allocator does not pre-allocate the entire specified GPU memory region, instead\n",
      "    starting small and growing as needed.\n",
      "    See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2\n",
      "\n",
      "gputouse: optional, default=None\n",
      "    Only for the TensorFlow engine (for the PyTorch engine see the ``torch_kwargs``:\n",
      "    you can use ``device``).\n",
      "    Natural number indicating the number of your GPU (see number in nvidia-smi).\n",
      "    If you do not have a GPU put None.\n",
      "    See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n",
      "\n",
      "autotune: bool, optional, default=False\n",
      "    Only for the TensorFlow engine.\n",
      "    Property of TensorFlow, somehow faster if ``False``\n",
      "    (as Eldar found out, see https://github.com/tensorflow/tensorflow/issues/13317).\n",
      "\n",
      "keepdeconvweights: bool, optional, default=True\n",
      "    Also restores the weights of the deconvolution layers (and the backbone) when\n",
      "    training from a snapshot. Note that if you change the number of bodyparts, you\n",
      "    need to set this to false for re-training.\n",
      "\n",
      "modelprefix: str, optional, default=\"\"\n",
      "    Directory containing the deeplabcut models to use when evaluating the network.\n",
      "    By default, the models are assumed to exist in the project folder.\n",
      "\n",
      "superanimal_name: str, optional, default =\"\"\n",
      "    Only for the TensorFlow engine. For the PyTorch engine, you need to specify\n",
      "    this through the ``weight_init`` when creating the training dataset.\n",
      "    Specified if transfer learning with superanimal is desired\n",
      "\n",
      "superanimal_transfer_learning: bool, optional, default = False.\n",
      "    Only for the TensorFlow engine. For the PyTorch engine, you need to specify\n",
      "    this through the ``weight_init`` when creating the training dataset.\n",
      "    If set true, the training is transfer learning (new decoding layer). If set\n",
      "    false, and superanimal_name is True, then the training is fine-tuning (reusing\n",
      "    the decoding layer)\n",
      "\n",
      "engine: Engine, optional, default = None.\n",
      "    The default behavior loads the engine for the shuffle from the metadata. You can\n",
      "    overwrite this by passing the engine as an argument, but this should generally\n",
      "    not be done.\n",
      "\n",
      "device: str, optional, default = None.\n",
      "    Only for the PyTorch engine. The device to run the training on (e.g. \"cuda:0\")\n",
      "\n",
      "snapshot_path: str or Path, optional, default = None.\n",
      "    Only for the PyTorch engine. The path to the pose model snapshot to resume training from.\n",
      "\n",
      "detector_path: str or Path, optional, default = None.\n",
      "    Only for the PyTorch engine. The path to the detector model snapshot to resume training from.\n",
      "\n",
      "batch_size: int, optional, default = None.\n",
      "    Only for the PyTorch engine. The batch size to use while training.\n",
      "\n",
      "detector_batch_size: int, optional, default = None.\n",
      "    Only for the PyTorch engine. The batch size to use while training the detector.\n",
      "\n",
      "detector_epochs: int, optional, default = None.\n",
      "    Only for the PyTorch engine. The number of epochs to train the detector for.\n",
      "\n",
      "detector_save_epochs: int, optional, default = None.\n",
      "    Only for the PyTorch engine. The number of epochs between each detector snapshot save.\n",
      "\n",
      "pose_threshold: float, optional, default = 0.1.\n",
      "    Only for the PyTorch engine. Used for memory-replay. Pseudo-predictions with confidence lower\n",
      "        than this threshold are discarded for memory-replay\n",
      "\n",
      "pytorch_cfg_updates: dict, optional, default = None.\n",
      "    A dictionary of updates to the pytorch config. The keys are the dot-separated\n",
      "    paths to the values to update in the config.\n",
      "    For example, to update the gpus to run the training on, you can use:\n",
      "    ```\n",
      "    pytorch_cfg_updates={\"runner.gpus\": [0,1,2,3]}\n",
      "    ```\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None\n",
      "\n",
      "Examples\n",
      "--------\n",
      "To train the network for first shuffle of the training dataset\n",
      "\n",
      ">>> deeplabcut.train_network('/analysis/project/reaching-task/config.yaml')\n",
      "\n",
      "To train the network for second shuffle of the training dataset\n",
      "\n",
      ">>> deeplabcut.train_network(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        shuffle=2,\n",
      "        keepdeconvweights=True,\n",
      "    )\n",
      "\n",
      "To train the network for shuffle created with a PyTorch engine, while overriding the\n",
      "number of epochs, batch size and other parameters.\n",
      "\n",
      ">>> deeplabcut.train_network(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        shuffle=1,\n",
      "        batch_size=8,\n",
      "        epochs=100,\n",
      "        save_epochs=10,\n",
      "        displayiters=50,\n",
      "    )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\mai-an.nguyen\\work\\deeplabcut\\deeplabcut\\compat.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b94721",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.train_network(\n",
    "    config_path,\n",
    "    shuffle=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fdd88",
   "metadata": {},
   "source": [
    "### Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2816b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | Path'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mShuffles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Iterable[int]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mplotting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshow_errors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcomparisonbodyparts\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | list[str]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgputouse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrescale\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mper_keypoint_evaluation\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msnapshots_to_evaluate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list[str] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpcutoff\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'float | list[float] | dict[str, float] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Engine | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mtorch_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Evaluates the network.\n",
      "\n",
      "Evaluates the network based on the saved models at different stages of the training\n",
      "network. The evaluation results are stored in the .h5 and .csv file under the\n",
      "subdirectory 'evaluation_results'. Change the snapshotindex parameter in the config\n",
      "file to 'all' in order to evaluate all the saved models.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "config : string\n",
      "    Full path of the config.yaml file.\n",
      "\n",
      "Shuffles: list, optional, default=[1]\n",
      "    List of integers specifying the shuffle indices of the training dataset.\n",
      "\n",
      "trainingsetindex: int or str, optional, default=0\n",
      "    Integer specifying which \"TrainingsetFraction\" to use.\n",
      "    Note that \"TrainingFraction\" is a list in config.yaml. This variable can also\n",
      "    be set to \"all\".\n",
      "\n",
      "plotting: bool or str, optional, default=False\n",
      "    Plots the predictions on the train and test images.\n",
      "    If provided it must be either ``True``, ``False``, ``\"bodypart\"``, or\n",
      "    ``\"individual\"``. Setting to ``True`` defaults as ``\"bodypart\"`` for\n",
      "    multi-animal projects.\n",
      "    If a detector is used, the predicted bounding boxes will also be plotted.\n",
      "\n",
      "show_errors: bool, optional, default=True\n",
      "    Display train and test errors.\n",
      "\n",
      "comparisonbodyparts: str or list, optional, default=\"all\"\n",
      "    The average error will be computed for those body parts only.\n",
      "    The provided list has to be a subset of the defined body parts.\n",
      "\n",
      "gputouse: int or None, optional, default=None\n",
      "    Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a\n",
      "    GPU put `None``.\n",
      "    See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n",
      "\n",
      "rescale: bool, optional, default=False\n",
      "    Evaluate the model at the ``'global_scale'`` variable (as set in the\n",
      "    ``pose_config.yaml`` file for a particular project). I.e. every image will be\n",
      "    resized according to that scale and prediction will be compared to the resized\n",
      "    ground truth. The error will be reported in pixels at rescaled to the\n",
      "    *original* size. I.e. For a [200,200] pixel image evaluated at\n",
      "    ``global_scale=.5``, the predictions are calculated on [100,100] pixel images,\n",
      "    compared to 1/2*ground truth and this error is then multiplied by 2!.\n",
      "    The evaluation images are also shown for the original size!\n",
      "\n",
      "modelprefix: str, optional, default=\"\"\n",
      "    Directory containing the deeplabcut models to use when evaluating the network.\n",
      "    By default, the models are assumed to exist in the project folder.\n",
      "\n",
      "per_keypoint_evaluation: bool, default=False\n",
      "    Compute the train and test RMSE for each keypoint, and save the results to\n",
      "    a {model_name}-keypoint-results.csv in the evaluation-results folder\n",
      "\n",
      "snapshots_to_evaluate: List[str], optional, default=None\n",
      "    List of snapshot names to evaluate (e.g. [\"snapshot-5000\", \"snapshot-7500\"]).\n",
      "\n",
      "pcutoff: float | list[float] | dict[str, float] | None, default=None\n",
      "    Only for the PyTorch engine. For the TensorFlow engine, please set the pcutoff\n",
      "    in the `config.yaml` file.\n",
      "    The cutoff to use for computing evaluation metrics. When `None` (default), the\n",
      "    cutoff will be loaded from the project config. If a list is provided, there\n",
      "    should be one value for each bodypart and one value for each unique bodypart\n",
      "    (if there are any). If a dict is provided, the keys should be bodyparts\n",
      "    mapping to pcutoff values for each bodypart. Bodyparts that are not defined\n",
      "    in the dict will have pcutoff set to 0.6.\n",
      "\n",
      "engine: Engine, optional, default = None.\n",
      "    The default behavior loads the engine for the shuffle from the metadata. You can\n",
      "    overwrite this by passing the engine as an argument, but this should generally\n",
      "    not be done.\n",
      "\n",
      "torch_kwargs:\n",
      "    You can add any keyword arguments for the deeplabcut.pose_estimation_pytorch\n",
      "    evaluate_network function here. These arguments are passed to the downstream\n",
      "    function. Available parameters are `snapshotindex`, which overrides the\n",
      "    `snapshotindex` parameter in the project configuration file. For top-down models\n",
      "    the `detector_snapshot_index` parameter can override the index of the detector\n",
      "    to use for evaluation in the project configuration file.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None\n",
      "\n",
      "Examples\n",
      "--------\n",
      "If you do not want to plot and evaluate with shuffle set to 1.\n",
      "\n",
      ">>> deeplabcut.evaluate_network(\n",
      "        '/analysis/project/reaching-task/config.yaml', Shuffles=[1],\n",
      "    )\n",
      "\n",
      "If you want to plot and evaluate with shuffle set to 0 and 1.\n",
      "\n",
      ">>> deeplabcut.evaluate_network(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        Shuffles=[0, 1],\n",
      "        plotting=True,\n",
      "    )\n",
      "\n",
      "If you want to plot assemblies for a maDLC project\n",
      "\n",
      ">>> deeplabcut.evaluate_network(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        Shuffles=[1],\n",
      "        plotting=\"individual\",\n",
      "    )\n",
      "\n",
      "If you have a PyTorch model for which you want to set a different p-cutoff for\n",
      "\"left_ear\" and \"right_ear\" bodyparts, and keep the one set in the project config\n",
      "for other bodyparts:\n",
      "\n",
      ">>> deeplabcut.evaluate_network(\n",
      ">>>     \"/analysis/project/reaching-task/config.yaml\",\n",
      ">>>     Shuffles=[0, 1],\n",
      ">>>     pcutoff={\"left_ear\": 0.8, \"right_ear\": 0.8},\n",
      ">>> )\n",
      "\n",
      "Note: This defaults to standard plotting for single-animal projects.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\mai-an.nguyen\\work\\deeplabcut\\deeplabcut\\compat.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "deeplabcut.evaluate_network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(\n",
    "    config_path,\n",
    "    Shuffles=[5],\n",
    "    plotting=False,\n",
    "    per_keypoint_evaluation=True,\n",
    "    snapshots_to_evaluate=['snapshot-175']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8535762d",
   "metadata": {},
   "source": [
    "### Analyze the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7faa861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze_videos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideos\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list[str]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideotype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgputouse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msave_as_csv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0min_random_order\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdestfolder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatchsize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcropping\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list[int] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mTFGPUinference\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdynamic\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'tuple[bool, float, int]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrobust_nframes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mallow_growth\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_shelve\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mauto_track\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_tracks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0manimal_names\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list[str] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcalibrate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0midentity_only\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_openvino\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Engine | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mtorch_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Makes prediction based on a trained network.\n",
      "\n",
      "The index of the trained network is specified by parameters in the config file\n",
      "(in particular the variable 'snapshotindex').\n",
      "\n",
      "The labels are stored as MultiIndex Pandas Array, which contains the name of\n",
      "the network, body part name, (x, y) label position in pixels, and the\n",
      "likelihood for each frame per body part. These arrays are stored in an\n",
      "efficient Hierarchical Data Format (HDF) in the same directory where the video\n",
      "is stored. However, if the flag save_as_csv is set to True, the data can also\n",
      "be exported in comma-separated values format (.csv), which in turn can be\n",
      "imported in many programs, such as MATLAB, R, Prism, etc.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "config: str\n",
      "    Full path of the config.yaml file.\n",
      "\n",
      "videos: list[str]\n",
      "    A list of strings containing the full paths to videos for analysis or a path to\n",
      "    the directory, where all the videos with same extension are stored.\n",
      "\n",
      "videotype: str, optional, default=\"\"\n",
      "    Checks for the extension of the video in case the input to the video is a\n",
      "    directory. Only videos with this extension are analyzed. If left unspecified,\n",
      "    videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.\n",
      "\n",
      "shuffle: int, optional, default=1\n",
      "    An integer specifying the shuffle index of the training dataset used for\n",
      "    training the network.\n",
      "\n",
      "trainingsetindex: int, optional, default=0\n",
      "    Integer specifying which TrainingsetFraction to use.\n",
      "    By default the first (note that TrainingFraction is a list in config.yaml).\n",
      "\n",
      "gputouse: int or None, optional, default=None\n",
      "    Only for the TensorFlow engine (for the PyTorch engine see the ``torch_kwargs``:\n",
      "    you can use ``device``).\n",
      "    Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a\n",
      "    GPU put ``None``.\n",
      "    See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n",
      "\n",
      "save_as_csv: bool, optional, default=False\n",
      "    Saves the predictions in a .csv file.\n",
      "\n",
      "in_random_order: bool, optional (default=True)\n",
      "    Whether or not to analyze videos in a random order.\n",
      "    This is only relevant when specifying a video directory in `videos`.\n",
      "\n",
      "destfolder: string or None, optional, default=None\n",
      "    Specifies the destination folder for analysis data. If ``None``, the path of\n",
      "    the video is used. Note that for subsequent analysis this folder also needs to\n",
      "    be passed.\n",
      "\n",
      "batchsize: int or None, optional, default=None\n",
      "    Currently not supported by the PyTorch engine.\n",
      "    Change batch size for inference; if given overwrites value in ``pose_cfg.yaml``.\n",
      "\n",
      "cropping: list or None, optional, default=None\n",
      "    List of cropping coordinates as [x1, x2, y1, y2].\n",
      "    Note that the same cropping parameters will then be used for all videos.\n",
      "    If different video crops are desired, run ``analyze_videos`` on individual\n",
      "    videos with the corresponding cropping coordinates.\n",
      "\n",
      "TFGPUinference: bool, optional, default=True\n",
      "    Only for the TensorFlow engine.\n",
      "    Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining\n",
      "    boosts out-of-domain robustness for pose estimation\" by Alexander Mathis,\n",
      "    Mert Yksekgnl, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis.\n",
      "    Source: https://arxiv.org/abs/1909.11229\n",
      "\n",
      "dynamic: tuple(bool, float, int) triple containing (state, det_threshold, margin)\n",
      "    If the state is true, then dynamic cropping will be performed. That means that\n",
      "    if an object is detected (i.e. any body part > detectiontreshold), then object\n",
      "    boundaries are computed according to the smallest/largest x position and\n",
      "    smallest/largest y position of all body parts. This  window is expanded by the\n",
      "    margin and from then on only the posture within this crop is analyzed (until the\n",
      "    object is lost, i.e. <detectiontreshold). The current position is utilized for\n",
      "    updating the crop window for the next frame (this is why the margin is important\n",
      "    and should be set large enough given the movement of the animal).\n",
      "\n",
      "modelprefix: str, optional, default=\"\"\n",
      "    Directory containing the deeplabcut models to use when evaluating the network.\n",
      "    By default, the models are assumed to exist in the project folder.\n",
      "\n",
      "robust_nframes: bool, optional, default=False\n",
      "    Evaluate a video's number of frames in a robust manner.\n",
      "    This option is slower (as the whole video is read frame-by-frame),\n",
      "    but does not rely on metadata, hence its robustness against file corruption.\n",
      "\n",
      "allow_growth: bool, optional, default=False.\n",
      "    Only for the TensorFlow engine.\n",
      "    For some smaller GPUs the memory issues happen. If ``True``, the memory\n",
      "    allocator does not pre-allocate the entire specified GPU memory region, instead\n",
      "    starting small and growing as needed.\n",
      "    See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2\n",
      "\n",
      "use_shelve: bool, optional, default=False\n",
      "    By default, data are dumped in a pickle file at the end of the video analysis.\n",
      "    Otherwise, data are written to disk on the fly using a \"shelf\"; i.e., a\n",
      "    pickle-based, persistent, database-like object by default, resulting in\n",
      "    constant memory footprint.\n",
      "\n",
      "The following parameters are only relevant for multi-animal projects:\n",
      "\n",
      "auto_track: bool, optional, default=True\n",
      "    By default, tracking and stitching are automatically performed, producing the\n",
      "    final h5 data file. This is equivalent to the behavior for single-animal\n",
      "    projects.\n",
      "\n",
      "    If ``False``, one must run ``convert_detections2tracklets`` and\n",
      "    ``stitch_tracklets`` afterwards, in order to obtain the h5 file.\n",
      "\n",
      "This function has 3 related sub-calls:\n",
      "\n",
      "identity_only: bool, optional, default=False\n",
      "    If ``True`` and animal identity was learned by the model, assembly and tracking\n",
      "    rely exclusively on identity prediction.\n",
      "\n",
      "calibrate: bool, optional, default=False\n",
      "    If ``True``, use training data to calibrate the animal assembly procedure. This\n",
      "    improves its robustness to wrong body part links, but requires very little\n",
      "    missing data.\n",
      "\n",
      "n_tracks: int or None, optional, default=None\n",
      "    Number of tracks to reconstruct. By default, taken as the number of individuals\n",
      "    defined in the config.yaml. Another number can be passed if the number of\n",
      "    animals in the video is different from the number of animals the model was\n",
      "    trained on.\n",
      "\n",
      "animal_names: list[str], optional\n",
      "    If you want the names given to individuals in the labeled data file, you can\n",
      "    specify those names as a list here. If given and `n_tracks` is None, `n_tracks`\n",
      "    will be set to `len(animal_names)`. If `n_tracks` is not None, then it must be\n",
      "    equal to `len(animal_names)`. If it is not given, then `animal_names` will\n",
      "    be loaded from the `individuals` in the project config.yaml file.\n",
      "\n",
      "use_openvino: str, optional\n",
      "    Only for the TensorFlow engine.\n",
      "    Use \"CPU\" for inference if OpenVINO is available in the Python environment.\n",
      "\n",
      "engine: Engine, optional, default = None.\n",
      "    The default behavior loads the engine for the shuffle from the metadata. You can\n",
      "    overwrite this by passing the engine as an argument, but this should generally\n",
      "    not be done.\n",
      "\n",
      "torch_kwargs:\n",
      "    Any extra parameters to pass to the PyTorch API, such as ``device`` which can\n",
      "    be used to specify the CUDA device to use for training.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DLCScorer: str\n",
      "    the scorer used to analyze the videos\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Analyzing a single video on Windows\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        'C:\\myproject\\reaching-task\\config.yaml',\n",
      "        ['C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'],\n",
      "    )\n",
      "\n",
      "Analyzing a single video on Linux/MacOS\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        ['/analysis/project/videos/reachingvideo1.avi'],\n",
      "    )\n",
      "\n",
      "Analyze all videos of type ``avi`` in a folder\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        ['/analysis/project/videos'],\n",
      "        videotype='.avi',\n",
      "    )\n",
      "\n",
      "Analyze multiple videos\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        [\n",
      "            '/analysis/project/videos/reachingvideo1.avi',\n",
      "            '/analysis/project/videos/reachingvideo2.avi',\n",
      "        ],\n",
      "    )\n",
      "\n",
      "Analyze multiple videos with ``shuffle=2``\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        [\n",
      "            '/analysis/project/videos/reachingvideo1.avi',\n",
      "            '/analysis/project/videos/reachingvideo2.avi',\n",
      "        ],\n",
      "        shuffle=2,\n",
      "    )\n",
      "\n",
      "Analyze multiple videos with ``shuffle=2``, save results as an additional csv file\n",
      "\n",
      ">>> deeplabcut.analyze_videos(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        [\n",
      "            '/analysis/project/videos/reachingvideo1.avi',\n",
      "            '/analysis/project/videos/reachingvideo2.avi',\n",
      "        ],\n",
      "        shuffle=2,\n",
      "        save_as_csv=True,\n",
      "    )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\mai-an.nguyen\\work\\deeplabcut\\deeplabcut\\compat.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "deeplabcut.analyze_videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e66226",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_to_analyze = Path(r\"video.mp4\")\n",
    "deeplabcut.analyze_videos(\n",
    "    config_path,\n",
    "    videos=[videos_to_analyze],\n",
    "    shuffle=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bf498",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.filterpredictions(\n",
    "    config_path,\n",
    "    video=[str(videos_to_analyze)],\n",
    "    shuffle=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820a870",
   "metadata": {},
   "source": [
    "#### Plot trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_filepath = r\"myproject/videos/video.h5\"\n",
    "df = pd.read_hdf(hdf_filepath)\n",
    "df = df.droplevel('scorer', axis=1)\n",
    "\n",
    "fs = 30  # video framerate\n",
    "df.index = pd.MultiIndex.from_arrays([df.index, df.index / fs], names=['frame','time']) # create a MultiIndex for rows with frame and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = df.melt(ignore_index=False).query('coords != \"likelihood\"').reset_index()\n",
    "\n",
    "fig = px.line(plot_df, x='time', y='value', color='bodyparts', line_dash='coords',\n",
    "              title=\"Bodyparts' coordinates over time\", labels={'value':'coordinate (pixels)', 'time':'time (s)'})\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a336c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = df.stack(['bodyparts'], future_stack=True).reset_index().drop(['frame', 'time'], axis=1)\n",
    "\n",
    "fig = px.scatter(plot_df, x='x', y='y', color='bodyparts', height=800,\n",
    "                 title=\"Positions of bodyparts in space\")\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "fig.update_xaxes(scaleanchor=\"y\", scaleratio=1)\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e96356",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = df.melt(ignore_index=False).query('coords == \"likelihood\"').reset_index()\n",
    "\n",
    "fig = px.line(plot_df, x='time', y='value', color='bodyparts', line_dash='coords',\n",
    "              title=\"Bodyparts' likelihood over time\", labels={'value':'likelihood', 'time':'time (s)'})\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f935632",
   "metadata": {},
   "source": [
    "#### Create video for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90756cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(\n",
    "    config_path,\n",
    "    videos=[str(videos_to_analyze)],\n",
    "    draw_skeleton=True,\n",
    "    color_by='bodypart',\n",
    "    shuffle=5,\n",
    "    filtered=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEPLABCUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
